<html><head>
<style>
body { font-size: 10px; }
.columns { column-count: 3; }
@media print { @page { size: A4; } }
</style>
</head><body>
<h1>Definitions</h1>
<div class="columns">
<p><b>Total Function</b>
A (total) function f : D → C (with sets D, C )
maps every value of its domain D
to exactly one value of its codomain C .
Example
▶ square : Z → Z with square(x) = x 2
▶ add : N2
▶ add R : R2 → R with add R(x, y ) = x + y
0 → N0 with add(x, y ) = x + y
</p>
<p><b>Partial Function</b>
A partial function f : X →p Y maps every value in X
to at most one value in Y .
If f does not map x ∈ X to any value in Y ,
then f is undefined for x.
Example
f : N0 × N0 →p N0 with
f (x, y ) =
(cid:40)
x − y
undefined otherwise
if y ≤ x
</p>
<p><b></b>A set is an unordered collection of distinct objects.
The set that does not contain any objects is the empty set ∅.
▶ A definition introduces an abbreviation.
▶ Whenever we say “set”, we could instead say “an unordered
collection of distinct objects” and vice versa.
▶ Definitions can also introduce notation.
</p>
<p><b>Alphabets, Words and Formal Languages</b>
An alphabet Σ is a finite non-empty set of symbols.
A word over Σ is a finite sequence of elements from Σ.
The empty word (the empty sequence of elements) is denoted by ε.
Σ∗ denotes the set of all words over Σ.
Σ+ (= Σ∗ \ {ε}) denotes the set of all non-empty words over Σ.
We write |w | for the length of a word w .
A formal language (over alphabet Σ) is a subset of Σ∗.
Example
Σ = {a, b}
Σ∗ = {ε, a, b, aa, ab, ba, bb, . . . }
|aba| = 3, |b| = 1, |ε| = 0
</p>
<p><b>Deterministic Finite Automata</b>
A deterministic finite automaton (DFA) is a 5-tuple
M = ⟨Q, Σ, δ, q0, F ⟩ where
▶ Q is the finite set of states
▶ Σ is the input alphabet
▶ δ : Q × Σ → Q is the transition function
▶ q0 ∈ Q is the start state
▶ F ⊆ Q is the set of accept states (or final states)
</p>
<p><b>Words Accepted by a DFA</b>
DFA M = ⟨Q, Σ, δ, q0, F ⟩ accepts the word w = a1 . . . an
if there is a sequence of states q′
n ∈ Q with
0, . . . , q′
0 = q0,
1 q′
2 δ(q′
3 q′
n ∈ F .
i−1, ai ) = q′
i for all i ∈ {1, . . . , n} and
</p>
<p><b>Language Recognized by a DFA</b>
Let M be a deterministic finite automaton.
The language recognized by M is defined as
L(M) = {w ∈ Σ∗ | w is accepted by M}.
</p>
<p><b>Nondeterministic Finite Automata</b>
A nondeterministic finite automaton (NFA) is a 5-tuple
M = ⟨Q, Σ, δ, q0, F ⟩ where
▶ Q is the finite set of states
▶ Σ is the input alphabet
▶ δ : Q × (Σ ∪ {ε}) → P(Q) is the transition function
(mapping to the power set of Q)
▶ q0 ∈ Q is the start state
▶ F ⊆ Q is the set of accept states
DFAs are (essentially) a special case of NFAs.
</p>
<p><b>ε-closure</b>
For NFA M = ⟨Q, Σ, δ, q0, F ⟩ and state q ∈ Q, state p is in the
ε-closure E (q) of q iff there is a sequence of states q′
0, . . . , q′
n with
1 q′
2 q′
3 q′
0 = q,
i ∈ δ(q′
n = p.
i−1, ε) for all i ∈ {1, . . . , n} and
q ∈ E (q) for every state q
</p>
<p><b>Words Accepted by an NFA</b>
NFA M = ⟨Q, Σ, δ, q0, F ⟩ accepts the word w = a1 . . . an
if there is a sequence of states q′
n ∈ Q with
0, . . . , q′
1 q′
2 q′
3 q′
0 ∈ E (q0),
i ∈ (cid:83)
n ∈ F .
q∈δ(q′
i−1,ai ) E (q) for all i ∈ {1, . . . , n} and
</p>
<p><b>Language Recognized by an NFA</b>
Let M be an NFA with input alphabet Σ.
The language recognized by M is defined as
L(M) = {w ∈ Σ∗ | w is accepted by M}.
</p>
<p><b>Grammars</b>
A grammar is a 4-tuple ⟨V , Σ, R, S⟩ with:
▶ V finite set of variables (nonterminal symbols)
▶ Σ finite alphabet of terminal symbols with V ∩ Σ = ∅
▶ R ⊆ (V ∪ Σ)∗V (V ∪ Σ)∗ × (V ∪ Σ)∗ finite set of rules
▶ S ∈ V start variable
A rule is sometimes also called a production or a production rule.
</p>
<p><b>Derivations</b>
Let ⟨V , Σ, R, S⟩ be a grammar. A word v ∈ (V ∪ Σ)∗ can be
derived from word u ∈ (V ∪ Σ)+ (written as u ⇒ v ) if
1 u = xyz, v = xy ′z with x, z ∈ (V ∪ Σ)∗ and
2
there is a rule y → y ′ ∈ R.
We write: u ⇒∗ v if v can be derived from u in finitely many steps
(i. e., by using n derivations for n ∈ N0).
</p>
<p><b>Languages</b>
The language generated by a grammar G = ⟨V , Σ, P, S⟩
L(G ) = {w ∈ Σ∗ | S ⇒∗ w }
is the set of all words from Σ∗ that can be derived from S
with finitely many rule applications.
</p>
<p><b>Chomsky Hierarchy</b>
▶ Every grammar is of type 0 (all rules allowed).
▶ Grammar is of type 1 (context-sensitive)
if all rules are of the form αBγ → αβγ
with B ∈ V and α, γ ∈ (V ∪ Σ)∗ and β ∈ (V ∪ Σ)+
▶ Grammar is of type 2 (context-free)
if all rules are of the form A → w ,
where A ∈ V and w ∈ (V ∪ Σ)+.
▶ Grammar is of type 3 (regular)
if all rules are of the form A → w ,
where A ∈ V and w ∈ Σ ∪ ΣV .
special case: rule S → ε is always allowed if S is the start variable
and never occurs on the right-hand side of any rule.
</p>
<p><b>Type 0–3 Languages</b>
A language L ⊆ Σ∗ is of type 0 (type 1, type 2, type 3)
if there exists a type-0 (type-1, type-2, type-3) grammar G
with L(G ) = L.
</p>
<p><b>Regular Grammars</b>
A regular grammar is a 4-tuple ⟨V , Σ, R, S⟩ with
▶ V finite set of variables (nonterminal symbols)
▶ Σ finite alphabet of terminal symbols with V ∩ Σ = ∅
▶ R ⊆ (V × (Σ ∪ ΣV )) ∪ {⟨S, ε⟩} finite set of rules
▶ if S → ε ∈ R, there is no X ∈ V , y ∈ Σ with X → yS ∈ R
▶ S ∈ V start variable.
Rule X → ε is only allowed if X = S and
S never occurs in the right-hand side of a rule.
</p>
<p><b>Regular Language</b>
A language L ⊆ Σ∗ is regular
if there exists a regular grammar G with L(G ) = L.
</p>
<p><b>Regular Grammars</b>
A regular grammar is a 4-tuple ⟨V , Σ, R, S⟩ with
▶ V finite set of variables (nonterminal symbols)
▶ Σ finite alphabet of terminal symbols with V ∩ Σ = ∅
▶ R ⊆ (V × (Σ ∪ ΣV )) ∪ {⟨S, ε⟩} finite set of rules
▶ if S → ε ∈ R, there is no X ∈ V , y ∈ Σ with X → yS ∈ R
▶ S ∈ V start variable.
Rule X → ε is only allowed if X = S and
S never occurs in the right-hand side of a rule.
How restrictive is this?
</p>
<p><b>Closure</b>
Let K be a class of languages.
Then K is closed. . .
▶ . . . under union if L, L′ ∈ K implies L ∪ L′ ∈ K
▶ . . . under intersection if L, L′ ∈ K implies L ∩ L′ ∈ K
▶ . . . under complement if L ∈ K implies ¯L ∈ K
▶ . . . under concatenation if L, L′ ∈ K implies LL′ ∈ K
▶ . . . under star if L ∈ K implies L∗ ∈ K
</p>
<p><b>Word Problem for Regular Languages</b>
The word problem P∈ for regular languages is:
Given:
Question:
regular grammar G with alphabet Σ
and word w ∈ Σ∗
Is w ∈ L(G )?
</p>
<p><b>Emptiness Problem for Regular Languages</b>
The emptiness problem P∅ for regular languages is:
Given:
Question:
regular grammar G
Is L(G ) = ∅?
</p>
<p><b>Finiteness Problem for Regular Languages</b>
The finiteness problem P∞ for regular languages is:
Given:
Question:
regular grammar G
Is |L(G )| < ∞?
</p>
<p><b>Intersection Problem for Regular Languages</b>
The intersection problem P∩ for regular languages is:
regular grammars G and G ′
Is L(G ) ∩ L(G ′) = ∅?
Given:
Question:
</p>
<p><b>Equivalence Problem for Regular Languages</b>
The equivalence problem P= for regular languages is:
regular grammars G and G ′
Is L(G ) = L(G ′)?
Given:
Question:
</p>
<p><b>Regular Expressions</b>
Regular expressions over an alphabet Σ are defined inductively:
▶ ∅ is a regular expression
▶ ε is a regular expression
▶ If a ∈ Σ, then a is a regular expression
If α and β are regular expressions, then so are:
▶ (αβ) (concatenation)
▶ (α|β) (alternative)
▶ (α∗) (Kleene closure)
</p>
<p><b>Language Described by a Regular Expression</b>
The language described by a regular expression γ, written L(γ),
is inductively defined as follows:
▶ If γ = ∅, then L(γ) = ∅.
▶ If γ = ε, then L(γ) = {ε}.
▶ If γ = a with a ∈ Σ, then L(γ) = {a}.
▶ If γ = (αβ), where α and β are regular expressions,
then L(γ) = L(α)L(β).
▶ If γ = (α|β), where α and β are regular expressions,
then L(γ) = L(α) ∪ L(β).
▶ If γ = (α∗) where α is a regular expression,
then L(γ) = L(α)∗.
Examples: blackboard
</p>
<p><b>Generalized Nondeterministic Finite Automata</b>
A generalized nondeterministic finite automaton (GNFA) is a
5-tuple M = ⟨Q, Σ, δ, qs , qa⟩ where
▶ Q is the finite set of states
▶ Σ is the input alphabet
▶ δ : (Q \ {qa}) × (Q \ {qs }) → RΣ is the transition function
(with RΣ the set of all regular expressions over Σ)
▶ qs ∈ Q is the start state
▶ qa ∈ Q is the accept state
</p>
<p><b>Words Accepted by a GNFA</b>
GNFA M = ⟨Q, Σ, δ, qs , qa⟩ accepts the word w
if w = w1 . . . wk , where each wi is in Σ∗
and a sequence of states q0, q1, . . . , qk ∈ Q exists with
1 q0 = qs ,
2
for each i, we have wi ∈ L(Ri ), where Ri = δ(qi−1, qi ), and
3 qk = qa.
</p>
<p><b>Context-free Grammar</b>
A context-free grammar is a 4-tuple ⟨V , Σ, P, S⟩ with
1 V finite set of variables,
2 Σ finite alphabet of terminal symbols (with V ∩ Σ = ∅),
3 P ⊆ (V × (V ∪ Σ)+) ∪ {⟨S, ε⟩} finite set of rules,
4
If S → ε ∈ P, then all other rules in V × ((V \ {S}) ∪ Σ)+.
5 S ∈ V start variable.
Rule X → ε is only allowed if X = S
and S never occurs on a right-hand side.
With regular grammars, this restriction could be lifted.
How about context-free grammars?
</p>
<p><b>Chomsky Normal Form</b>
A context-free grammar G is in Chomsky normal form
(CNF) if all rules have one of the following three forms:
▶ A → BC with variables A, B, C , or
▶ A → a with variable A, terminal symbol a, or
▶ S → ε with start variable S.
in short:
rule set P ⊆ (V × (V ′V ′ ∪ Σ)) ∪ {⟨S, ε⟩} with V ′ = V \ {S}
</p>
<p><b>Push-down Automaton</b>
A push-down automaton (PDA) is a 6-tuple
M = ⟨Q, Σ, Γ, δ, q0, F ⟩ with
▶ Q finite set of states
▶ Σ the input alphabet
▶ Γ the stack alphabet
▶ δ : Q × (Σ ∪ {ε}) × (Γ ∪ {ε}) → P(Q × (Γ ∪ {ε})) the
transition function
▶ q0 ∈ Q the start state
▶ F ⊆ Q is the set of accept states
</p>
<p><b></b>A PDA M = ⟨Q, Σ, Γ, δ, q0, F ⟩ accepts input w
if it can be written as w = w1w2 . . . wm where each wi ∈ Σ ∪ {ε}
and sequences of states r0, r1, . . . , rm ∈ Q and
strings s0, s1, . . . , sm ∈ Γ∗ exist
that satisfy the following three conditions:
1
r0 = q0 and s0 = ε
2 For i = 0, . . . , m − 1, we have (ri+1, b) ∈ δ(ri , wi+1, a),
where si = at and si+1 = bt for some a, b ∈ Γ ∪ {ε} and
t ∈ Γ∗.
rm ∈ F
3
The strings si represent the sequence of stack contents.
</p>
<p><b>Language Recognized by an NFA</b>
Let M be a PDA with input alphabet Σ.
The language recognized by M is defined as
L(M) = {w ∈ Σ∗ | w is accepted by M}.
</p>
<p><b>Word Problem for Context-free Languages</b>
The word problem P∈ for context-free languages is:
Given:
Question:
context-free grammar G with alphabet Σ
and word w ∈ Σ∗
Is w ∈ L(G )?
</p>
<p><b>Emptiness Problem for Context-free Languages</b>
The emptiness problem P∅ for context-free languages is:
Given:
Question:
context-free grammar G
Is L(G ) = ∅?
</p>
<p><b>Finiteness Problem for Context-free Languages</b>
The finiteness problem P∞ for context-free languages is:
Given:
Question:
context-free grammar G
Is |L(G )| < ∞?
</p>
<p><b>Intersection Problem for Context-free Languages</b>
The intersection problem P∩ for context-free languages is:
context-free grammars G and G ′
Is L(G ) ∩ L(G ′) = ∅?
Given:
Question:
</p>
<p><b>Equivalence Problem for Context-free Languages</b>
The equivalence problem P= for context-free languages is:
context-free grammars G and G ′
Is L(G ) = L(G ′)?
Given:
Question:
</p>
<p><b>Deterministic Turing Machine</b>
A (deterministic) Turing machine (DTM) is given by a 7-tuple
M = ⟨Q, Σ, Γ, δ, q0, qaccept, qreject⟩, where
Q, Σ, Γ are all finite sets and
▶ Q is the set of states,
▶ Σ is the input alphabet, not containing the blank symbol □,
▶ Γ is the tape alphabet, where □ ∈ Γ and Σ ⊆ Γ,
▶ δ : (Q \ {qaccept, qreject}) × Γ → Q × Γ × {L, R} is the
transition function,
▶ q0 ∈ Q is the start state,
▶ qaccept ∈ Q is the accept state,
▶ qreject ∈ Q is the reject state, where qaccept ̸= qreject.
</p>
<p><b>Configuration of a Turing Machine</b>
A configuration of a Turing machine
M = ⟨Q, Σ, Γ, δ, q0, qaccept, qreject⟩
is given by a triple c ∈ Γ∗ × Q × Γ+.
Configuration ⟨w1, q, w2⟩ intuitively means that
▶ the non-empty or already visited part of the tape
contains the word w1w2,
▶ the read-write head is on the first symbol of w2, and
▶ the TM is in state q.
</p>
<p><b>Transition/Step of a Turing Machine</b>
A DTM M = ⟨Q, Σ, Γ, δ, q0, qaccept, qreject⟩ transitions
from configuration c to configuration c ′ in one step (c ⊢M c ′)
according to the following rules:
▶ ⟨a1 . . . am, q, b1 . . . bn⟩ ⊢M ⟨a1 . . . amc, q′, b2 . . . bn⟩
if δ(q, b1) = ⟨q′, c, R⟩, m ≥ 0, n ≥ 2
▶ ⟨a1 . . . am, q, b1⟩ ⊢M ⟨a1 . . . amc, q′, □⟩
if δ(q, b1) = ⟨q′, c, R⟩, m ≥ 0
▶ ⟨a1 . . . am, q, b1 . . . bn⟩ ⊢M ⟨a1 . . . am−1, q′, amcb2 . . . bn⟩
if δ(q, b1) = ⟨q′, c, L⟩, m ≥ 1, n ≥ 1
▶ ⟨ε, q, b1 . . . bn⟩ ⊢M ⟨ε, q′, cb2 . . . bn⟩
if δ(q, b1) = ⟨q′, c, L⟩, n ≥ 1
</p>
<p><b>Words Accepted by a DTM</b>
DTM M = ⟨Q, Σ, Γ, δ, q0, qaccept, qreject⟩ accepts the word
w = a1 . . . an if there is a sequence of configurations c0, . . . , ck with
1 c0 is the start configuration of M on input w ,
2 ci ⊢M ci+1 for all i ∈ {0, . . . , k − 1}, and
3 ck is an accepting configuration,
i. e., a configuration with state qaccept.
</p>
<p><b>Turing-recognizable Language</b>
We call a language Turing-recognizable if some deterministic
Turing machine recognizes it.
</p>
<p><b>Turing-decidable Language</b>
We call a language Turing-decidable (or decidable) if some
deterministic Turing machine decides it.
</p>
<p><b>Deterministic Turing Machine</b>
A (deterministic) Turing machine (DTM) is given by a 7-tuple
M = ⟨Q, Σ, Γ, δ, q0, qaccept, qreject⟩, where Q, Σ, Γ are finite and
▶ Q is the set of states,
▶ Σ is the input alphabet, not containing the blank symbol □,
▶ Γ is the tape alphabet, where □ ∈ Γ and Σ ⊆ Γ,
▶ δ : (Q \ {qaccept, qreject}) × Γ → Q × Γ × {L, R} is the
transition function,
▶ q0 ∈ Q is the start state,
▶ qaccept ∈ Q is the accept state,
▶ qreject ∈ Q is the reject state, where qaccept ̸= qreject.
Deterministic TM with a single tape that is infinite at one side.
</p>
<p><b>Context-sensitive Grammar</b>
A context-sensitive grammar is a 4-tuple ⟨V , Σ, R, S⟩ with
▶ V finite set of variables (nonterminal symbols)
▶ Σ finite alphabet of terminal symbols with V ∩ Σ = ∅
▶ R ⊆ (V ∪ Σ)∗V (V ∪ Σ)∗ × (V ∪ Σ)∗ finite set of rules,
where all rules are of the form αBγ → αβγ
with B ∈ V and α, γ ∈ (V ∪ Σ)∗ and β ∈ (V ∪ Σ)+.
Exception: S → ε is allowed if S never occurs on the
right-hand side of a rule.
▶ S ∈ V start variable.
</p>
<p><b>Linear Bounded Automata</b>
An NTM M = ⟨Q, Σ, Γ, δ, q0, qaccept, qreject⟩
is called a linear bounded automaton (LBA)
if for all q ∈ Q \ {qaccept, qreject} and all transition rules
⟨q′, c, y ⟩ ∈ δ(q, □) we have c = □.
</p>
<p><b>Turing Machine Encoded by a Word</b>
For all w ∈ {0, 1}∗:
Mw =
if w is the encoding of some DTM M ′
(cid:40)
M ′
(cid:98)M otherwise
</p>
<p><b>Halting Problem</b>
The halting problem is the language
H = {w #x ∈ {0, 1, #}∗ | w , x ∈ {0, 1}∗,
Mw started on x terminates}
“Does the computation of the TM encoded by w halt on input x?”
“Does a given piece of code terminate on a given input?”
</p>
<p><b>Function Computed by a Turing Machine</b>
A DTM M = ⟨Q, Σ, Γ, δ, q0, qaccept, qreject⟩ computes the (partial)
function f : Σ∗ →p Σ∗ for which for all x, y ∈ Σ∗:
f (x) = y iff ⟨ε, q0, x⟩ ⊢∗ ⟨ε, qaccept, y □ . . . □⟩.
(special case: initial configuration ⟨ε, q0, □⟩ if x = ε)
▶ What happens if the computation does not reach qaccept?
▶ What happens if symbols from Γ \ Σ (e. g., □) occur in y ?
▶ What happens if the read-write head is not at the first tape
cell when accepting?
▶ Is f uniquely defined by this definition? Why?
</p>
<p><b>Turing-Computable, f : Σ∗ →p Σ∗</b>
A (partial) function f : Σ∗ →p Σ∗ is called Turing-computable
if a DTM that computes f exists.
</p>
<p><b>Encoded Function</b>
Let f : Nk
The encoded function f code of f is the partial function
f code : Σ∗ →p Σ∗ with Σ = {0, 1, #} and f code(w ) = w ′ iff
0 →p N0 be a (partial) function.
▶ there are n1, . . . , nk , n′ ∈ N0 such that
▶ f (n1, . . . , nk ) = n′,
▶ w = bin(n1)# . . . #bin(nk ) and
▶ w ′ = bin(n′).
Here bin : N0 → {0, 1}∗ is the binary encoding
(e. g., bin(5) = 101).
Example: f (5, 2, 3) = 4 corresponds to f code(101#10#11) = 100.
</p>
<p><b></b>A (partial) function f : Nk
if a DTM that computes f code exists.
0 →p N0)
0 →p N0 is called Turing-computable
</p>
<p><b>Reduction</b>
Let A ⊆ Σ∗ and B ⊆ Γ∗ be languages, and let f : Σ∗ → Γ∗
be a total and computable function such that for all x ∈ Σ∗:
x ∈ A if and only if
f (x) ∈ B.
Then we say that A can be reduced to B (in symbols: A ≤ B),
and f is called a reduction from A to B.
</p>
<p><b>Halting Problem</b>
The halting problem is the language
H = {w #x ∈ {0, 1, #}∗ | w , x ∈ {0, 1}∗,
Mw started on x terminates}
</p>
<p><b>Halting Problem on the Empty Tape</b>
The halting problem on the empty tape is the language
H0 = {w ∈ {0, 1}∗ | Mw started on ε terminates}.
Note: H0 is Turing-recognizable. (Why?)
</p>
<p><b>Post Correspondence Problem PCP</b>
Given: Finite sequence of pairs of words
(t1, b1), (t2, b2), . . . , (tk , bk ), where ti , bi ∈ Σ+
(for an arbitrary alphabet Σ)
Question:
Is there a sequence
i1, i2, . . . , in ∈ {1, . . . , k}, n ≥ 1,
with ti1ti2 . . . tin = bi1bi2 . . . bin ?
A solution of the correspondence problem is such a sequence
i1, . . . , in, which we call a match.
</p>
<p><b>new problem P</b>
The problem P is the language
P = {⟨⟨I⟩⟩ | I is an instance with the required property}.
</p>
<p><b>Post Correspondence Problem PCP</b>
The Post Correspondence Problem PCP is the set
PCP = {w | w encodes a sequence of pairs of words
(t1, b1), (t2, b2), . . . , (tk , bk ), for which there is a
sequence i1, i2, . . . , in ∈ {1, . . . , k}
such that ti1ti2 . . . tin = bi1bi2 . . . bin }.
</p>
<p><b>Modified Post Correspondence Problem MPCP</b>
Given: Sequence of word pairs as for PCP
Question:
Is there a match i1, i2, . . . , in ∈ {1, . . . , k}
with i1 = 1?
</p>
<p><b>Arithmetic Formula</b>
An arithmetic formula is a closed predicate logic formula using
▶ constant symbols 0 and 1,
▶ function symbols + and ·, and
▶ equality (=) as the only relation symbols.
It is called true if it is true under the usual interpretation
of 0, 1, + and · over N0.
Beispiel: ∀x∃y ∀z(((x · y ) = z) ∧ ((1 + x) = (x · y )))
</p>
<p><b>Hamilton Cycle</b>
Let G = ⟨V , E ⟩ be a (directed or undirected) graph.
A Hamilton cycle of G is a sequence of vertices in V ,
π = ⟨v0, . . . , vn⟩, with the following properties:
▶ π is a path: there is an edge from vi to vi+1 for all 0 ≤ i < n
▶ π is a cycle: v0 = vn
▶ π is simple: vi
▶ π is Hamiltonian: all nodes of V are included in π
̸= vj for all i ̸= j with i, j < n
</p>
<p><b>Runtime of a DTM</b>
Let M be a DTM that halts on all inputs. The running time or
time complexity of M if the function f : N → N, where f (n) is the
maximum number of steps that M uses on any input of length n.
We say that
▶ M runs in time f and that
▶ M is an f time Turing machine.
</p>
<p><b>Big-O</b>
Let f and g be functions f , g : N → R+.
We say that f ∈ O(g ) if positive integers c and n0 exist such that
for every integer n ≥ n0
f (n) ≤ cg (n).
</p>
<p><b>P</b>
P is the class of languages that are decidable in polynomial time by
a deterministic single-tape Turing machine. In other words,
P =
(cid:91)
k
TIME(nk ).
</p>
<p><b>Runtime of a NTM</b>
Let M be a NTM that is a decider, i. e. all its computation
branches halt on all inputs.
The running time or time complexity of M if the function
f : N → N, where f (n) is the maximum number of steps that M
uses on any branch of its computation on any input of length n.
</p>
<p><b>NP</b>
NP is the class of languages that are decidable in polynomial time
by a non-deterministic single-tape Turing machine. In other words,
NP =
(cid:91)
k
NTIME(nk ).
</p>
<p><b>Polynomial Reduction</b>
Let A ⊆ Σ∗ and B ⊆ Γ∗ be decision problems.
We say that A can be polynomially reduced to B,
written A ≤p B, if there is a function f : Σ∗ → Γ∗ such that:
▶ f can be computed in polynomial time by a DTM
▶ i. e., there is a polynomial p and a DTM M such that M
computes f (w ) in at most p(|w |) steps given input w ∈ Σ∗
▶ f reduces A to B
▶ i. e., for all w ∈ Σ∗: w ∈ A iff f (w ) ∈ B
f is called a polynomial reduction from A to B
</p>
<p><b>Hamilton Cycle</b>
A Hamilton cycle of G is a sequence of vertices in V ,
π = ⟨v0, . . . , vn⟩, with the following properties:
▶ π is a path: there is an edge from vi to vi+1 for all 0 ≤ i < n
▶ π is a cycle: v0 = vn
▶ π is simple: vi
▶ π is Hamiltonian: all nodes of V are included in π
̸= vj for all i ̸= j with i, j < n
</p>
<p><b>TSP</b>
TSP (traveling salesperson problem) is the following
decision problem:
▶ Given: finite set S ̸= ∅ of cities, symmetric cost function
cost : S × S → N0, cost bound K ∈ N0
▶ Question: Is there a tour with total cost at most K , i. e.,
a permutation ⟨s1, . . . , sn⟩ of the cities with
(cid:80)n−1
i=1 cost(si , si+1) + cost(sn, s1) ≤ K ?
</p>
<p><b>HamiltonianCompletion</b>
HamiltonianCompletion is the following decision problem:
▶ Given: undirected graph G = ⟨V , E ⟩, number k ∈ N0
▶ Question: Can G be extended with at most k edges such that
the resulting graph has a Hamilton cycle?
Show that
HamiltonCycle ≤p HamiltonianCompletion.
</p>
<p><b>NP-Hard, NP-Complete</b>
Let B be a decision problem.
B is called NP-hard if A ≤p B for all problems A ∈ NP.
B is called NP-complete if B ∈ NP and B is NP-hard.
</p>
<p><b>Polynomial Reduction</b>
Let A ⊆ Σ∗ and B ⊆ Γ∗ be decision problems.
We say that A can be polynomially reduced to B,
written A ≤p B, if there is a function f : Σ∗ → Γ∗ such that:
▶ f can be computed in polynomial time by a DTM
▶ f reduces A to B
▶ i. e., for all w ∈ Σ∗: w ∈ A iff f (w ) ∈ B
f is called a polynomial reduction from A to B
Transitivity of ≤p: If A ≤p B and B ≤p C , then A ≤p C .
</p>
<p><b>SAT</b>
The problem SAT (satisfiability) is defined as follows:
Given: a propositional logic formula φ
Question: Is φ satisfiable,
i.e. is there a variable assignment I such that I |= φ?
</p>
<p><b>SAT</b>
The problem SAT (satisfiability) is defined as follows:
Given: a propositional logic formula φ
Question: Is φ satisfiable?
</p>
<p><b>3SAT</b>
The problem 3SAT is defined as follows:
Given: a propositional logic formula φ in conjunctive normal form
with at most three literals per clause
Question: Is φ satisfiable?
</p>
<p><b>Clique</b>
The problem Clique is defined as follows:
Given: undirected graph G = ⟨V , E ⟩, number K ∈ N0
Question: Does G have a clique of size at least K ,
i. e., a set of vertices C ⊆ V with |C | ≥ K
and {u, v } ∈ E for all u, v ∈ C with u ̸= v ?
</p>
<p><b>IndSet</b>
The problem IndSet is defined as follows:
Given: undirected graph G = ⟨V , E ⟩, number K ∈ N0
Question: Does G have an independent set of size at least K ,
i. e., a set of vertices I ⊆ V with |I | ≥ K
and {u, v } /∈ E for all u, v ∈ I with u ̸= v ?
</p>
<p><b>VertexCover</b>
The problem VertexCover is defined as follows:
Given: undirected graph G = ⟨V , E ⟩, number K ∈ N0
Question: Does G have a vertex cover of size at most K ,
i. e., a set of vertices C ⊆ V with |C | ≤ K and {u, v } ∩ C ̸= ∅
for all {u, v } ∈ E ?
</p>
<p><b>Reminder: DirHamiltonCycle</b>
The problem DirHamiltonCycle is defined as follows:
Given: directed graph G = ⟨V , E ⟩
Question: Does G contain a Hamilton cycle?
</p>
<p><b>Reminder: HamiltonCycle</b>
The problem HamiltonCycle is defined as follows:
Given: undirected graph G = ⟨V , E ⟩
Question: Does G contain a Hamilton cycle?
</p>
<p><b>SubsetSum</b>
The problem SubsetSum is defined as follows:
Given: numbers a1, . . . , ak ∈ N0 and b ∈ N0
Question: Is there a subset J ⊆ {1, . . . , k} with (cid:80)
i∈J ai = b?
Which of the following instances have a subset with
the required sum?
1 numbers: 1, 3, 3, 4, 5
b = 11
2 numbers: 1, 1001, 10, 1010, 100, 1100, 1000, 2000
b = 4111
</p>
<p><b>Partition</b>
The problem Partition is defined as follows:
Given: numbers a1, . . . , ak ∈ N0
Question: Is there a subset J ⊆ {1, . . . , k}
with (cid:80)
i∈J ai = (cid:80)
i∈{1,...,k}\J ai ?
Is there a partition of numbers 3, 6, 6, 10, 15?
</p>
<p><b>BinPacking</b>
The problem BinPacking is defined as follows:
Given: bin size b ∈ N0, number of bins k ∈ N0,
objects a1, . . . , an ∈ N0
Question: Do the objects fit into the bins?
Formally: is there a mapping f : {1, . . . , n} → {1, . . . , k}
with (cid:80)
i∈{1,...,n} with f (i)=j ai ≤ b for all 1 ≤ j ≤ k?
Can you pack objects 1, 3, 3, 5, 8, 9 into 3 bins of size 10?
</p>
<p><b>coNP</b>
coNP is the set of all languages L for which ¯L ∈ NP.
Example: The complement of SAT is in coNP.
</p>
<p><b>Hardness and Completeness</b>
Let C be a complexity class.
A problem Y is called C-hard if X ≤p Y for all problems X ∈ C.
Y is called C-complete if Y ∈ C and Y is C-hard.
Example (Tautology)
The following problem Tautology is coNP-complete:
Given: a propositional logic formula φ
Question: Is φ valid, i.e. is it true under all variable assignments?
</p>
<p><b>Time Complexity Classes TIME and NTIME</b>
Let t : N → R+ be a function.
The time complexity class TIME(t(n)) is the collection of all
languages that are decidable by an O(t) time Turing machine, and
NTIME(t(n)) is the collection of all languages that are decidable
by an O(t) time nondeterministic Turing machine.
▶ TIME(f ): all languages accepted by a DTM in time f .
▶ NTIME(f ): all languages accepted by a NTM in time f .
▶ P = (cid:83)
▶ NP = (cid:83)
k∈N TIME(nk )
k∈N NTIME(nk )
</p>
<p><b>LOOP Program</b>
LOOP programs are inductively defined as follows:
▶ xi := xj + c is a LOOP program
for every i, j, c ∈ N0 (addition)
▶ xi := xj − c is a LOOP program
for every i, j, c ∈ N0 (modified subtraction)
▶ If P1 and P2 are LOOP programs,
then so is P1;P2 (composition)
▶ If P is a LOOP program, then so is
LOOP xi DO P END for every i ∈ N0 (LOOP loop)
</p>
<p><b>Semantics of LOOP Programs</b>
A LOOP program computes a k-ary function
f : Nk
0 → N0. The computation of f (n1, . . . , nk ) works as follows:
Initially, the variables x1, . . . , xk hold the values n1, . . . , nk .
All other variables hold the value 0.
1
2 During computation, the program modifies the variables
as described on the following slides.
3 The result of the computation (f (n1, . . . , nk )) is
the value of x0 after the execution of the program.
</p>
<p><b>Semantics of LOOP Programs</b>
effect of xi := xj + c:
▶ The variable xi is assigned the current value of xj plus c.
▶ All other variables retain their value.
</p>
<p><b>Semantics of LOOP Programs</b>
effect of xi := xj − c:
▶ The variable xi is assigned the current value of xj minus c
if this value is non-negative.
▶ Otherwise xi is assigned the value 0.
▶ All other variables retain their value.
</p>
<p><b>Semantics of LOOP Programs</b>
effect of P1; P2:
▶ First, execute P1.
Then, execute P2 (on the modified variable values).
</p>
<p><b>Semantics of LOOP Programs</b>
effect of LOOP xi DO P END:
▶ Let m be the value of variable xi at the start of execution.
▶ The program P is executed m times in sequence.
</p>
<p><b>LOOP-Computable</b>
A function f : Nk
if a LOOP program that computes f exists.
0 →p N0 is called LOOP-computable
Note: non-total functions are never LOOP-computable.
Note: (Why not?)
</p>
<p><b>WHILE Program</b>
WHILE programs are inductively defined as follows:
▶ xi := xj + c is a WHILE program
for every i, j, c ∈ N0 (addition)
▶ xi := xj − c is a WHILE program
for every i, j, c ∈ N0 (modified subtraction)
▶ If P1 and P2 are WHILE programs,
then so is P1;P2 (composition)
▶ If P is a WHILE program, then so is
WHILE xi
̸= 0 DO P END for every i ∈ N0 (WHILE loop)
</p>
<p><b>Semantics of WHILE Programs</b>
The semantics of WHILE programs is defined
exactly as for LOOP programs.
effect of WHILE xi
̸= 0 DO P END:
▶ If xi holds the value 0, program execution finishes.
▶ Otherwise execute P.
▶ Repeat these steps until execution finishes
(potentially infinitely often).
</p>
<p><b>WHILE-Computable</b>
A function f : Nk
if a WHILE program that computes f exists.
0 →p N0 is called WHILE-computable
</p>
<p><b>Ackermann function</b>
The Ackermann function a : N2
0 → N0 is defined as follows:
a(0, y ) = y + 1
a(x, 0) = a(x − 1, 1)
for all y ≥ 0
for all x > 0
a(x, y ) = a(x − 1, a(x, y − 1))
for all x, y > 0
Note: the recursion in the definition is bounded,
Note: so this defines a total function.
</p>
<p><b>GOTO Program</b>
A GOTO program is given by a finite sequence
L1 : A1, L2 : A2, . . . , Ln : An
of labels and statements.
Statements are of the following form:
▶ xi := xj + c for every i, j, c ∈ N0 (addition)
▶ xi := xj − c for every i, j, c ∈ N0 (modified subtraction)
▶ HALT (end of program)
▶ GOTO Lj for 1 ≤ j ≤ n (jump)
▶ IF xi = c THEN GOTO Lj for i, c ∈ N0,
1 ≤ j ≤ n (conditional jump)
</p>
<p><b>Semantics of GOTO Programs</b>
▶ Input, output and variables work exactly
as in LOOP and WHILE programs.
▶ Addition and modified subtraction work exactly
as in LOOP and WHILE programs.
▶ Execution begins with the statement A1.
▶ After executing Ai , the statement Ai+1 is executed.
(If i = n, execution finishes.)
▶ exceptions to the previous rule:
▶ HALT stops the execution of the program.
▶ After GOTO Lj execution continues with statement Aj .
▶ After IF xi = c THEN GOTO Lj execution continues
with Aj if variable xi currently holds the value c.
</p>
<p><b>GOTO-Computable</b>
A function f : Nk
if a GOTO program that computes f exists.
0 → N0 is called GOTO-computable
</p>
</div>
<h1>Theorems</h1>
<div class="columns">
<p><b>distributivity</b>
For all sets A, B, C : A ∩ (B ∪ C ) = (A ∩ B) ∪ (A ∩ C ).
</p>
<p><b></b>There are infinitely many prime numbers.
</p>
<p><b></b>For all n ∈ N0 with n ≥ 1: (cid:80)n
k=1(2k − 1) = n2
</p>
<p><b>Rabin, Scott</b>
Every language recognized by an NFA is also recognized by a DFA.
The proof of the theorem is constructive and shows how we can
convert an NFA to an equivalent DFA. Let’s first have a look at
the idea by means of an example (on the blackboard).
</p>
<p><b>Rabin, Scott</b>
Every language recognized by an NFA is also recognized by a DFA.
</p>
<p><b></b>For every grammar G = ⟨V , Σ, R, S⟩ there is a grammar
G ′ = ⟨V ′, Σ, R ′, S⟩ with rules
R ′ ⊆ (V ′ ∪ Σ)∗V ′(V ′ ∪ Σ)∗ × (V ′ \ {S} ∪ Σ)∗ such that
L(G ) = L(G ′).
Note: this theorem is true for all grammars.
</p>
<p><b></b>For every grammar G with rules R ⊆ V × (Σ ∪ ΣV ∪ {ε})
there is a regular grammar G ′ with L(G ) = L(G ′).
</p>
<p><b></b>Every language recognized by a DFA is regular (type 3).
</p>
<p><b></b>For every regular grammar G there is an NFA M
with L(G ) = L(M).
</p>
<p><b></b>The regular languages are closed under union.
</p>
<p><b></b>The regular languages are closed under concatenation.
</p>
<p><b></b>The regular languages are closed under star.
</p>
<p><b></b>The regular languages are closed under complement.
</p>
<p><b></b>The regular languages are closed under intersection.
</p>
<p><b></b>The regular languages are closed under:
▶ union
▶ intersection
▶ complement
▶ concatenation
▶ star
</p>
<p><b></b>The word problem for regular languages is decidable.
</p>
<p><b></b>The emptiness problem for regular languages is decidable.
</p>
<p><b></b>The finiteness problem for regular languages is decidable.
</p>
<p><b></b>The intersection problem for regular languages is decidable.
</p>
<p><b></b>The equivalence problem for regular languages is decidable.
</p>
<p><b></b>Every finite language can be described by a regular expression.
</p>
<p><b></b>For every language that can be described by a regular expression,
there is an NFA that accepts it.
</p>
<p><b></b>Every language recognized by a DFA can be described
by a regular expression.
We can prove this using a generalization of NFAs.
We specify the corresponding algorithm.
</p>
<p><b>Kleene</b>
The set of languages that can be described by regular expressions
is exactly the set of regular languages.
This follows directly from the previous two theorems.
</p>
<p><b>Pumping Lemma</b>
If L is a regular language then there is a number p ∈ N
(a pumping number for L) such that all words x ∈ L with |x| ≥ p
can be split into x = uvw with the following properties:
1
2
|v | ≥ 1,
|uv | ≤ p, and
3 uv i w ∈ L for all i = 0, 1, 2, . . . .
</p>
<p><b></b>For every grammar G = ⟨V , Σ, P, S⟩ there is a grammar
G ′ = ⟨V ′, Σ, P ′, S⟩ with rules P ′ ⊆ (V ′ ∪ Σ)+ × (V ′ \ {S} ∪ Σ)∗
such that L(G ) = L(G ′).
In the proof we constructed a suitable grammar, where the rules in
P ′ were not fundamentally different from the rules in P:
▶ for rules from V × (V ∪ Σ)+, we only introduced additional
rules from V ′ × (V ′ ∪ Σ)+, and
▶ for rules from V × ε, we only introduced rules from V ′ × ε,
where V ′ = V ∪ {S ′} for some new variable S ′ ̸∈ V .
</p>
<p><b></b>For every grammar G with rules P ⊆ V × (V ∪ Σ)∗
there is a context-free grammar G ′ with L(G ) = L(G ′).
</p>
<p><b></b>For every context-free grammar G there is a context-free grammar
G ′ in Chomsky normal form with L(G ) = L(G ′).
</p>
<p><b></b>A language L is context-free if and only if
L is recognized by a push-down automaton.
</p>
<p><b></b>The context-free languages are closed under:
▶ union
▶ concatenation
▶ star
</p>
<p><b></b>The context-free languages are not closed under:
▶ intersection
▶ complement
</p>
<p><b></b>The word problem P∈ for context-free languages is decidable.
</p>
<p><b></b>The emptiness problem for context-free languages is decidable.
</p>
<p><b></b>The finiteness problem for context-free languages is decidable.
We omit the proof. A possible proof uses the pumping lemma
for context-free languages.
</p>
<p><b></b>The equivalence problem for context-free languages
and the intersection problem for context-free languages
are not decidable.
We cannot show this with the means currently available,
but we will get back to this in Part C (computability theory).
</p>
<p><b></b>Every multitape TM has an equivalent single-tape TM.
</p>
<p><b></b>A language is Turing-recognizable iff some multitape Turing
machine recognizes it.
</p>
<p><b></b>Every nondeterministic Turing machine has an equivalent
deterministic Turing machine.
</p>
<p><b></b>A language is Turing-recognizable iff some nondeterministic Turing
machine recognizes it.
</p>
<p><b></b>The languages that can be recognized by linear bounded automata
are exactly the context-sensitive (type-1) languages.
Without proof.
proof sketch for grammar ⇒ NTM direction:
▶ computation of the NTM follows the production of the word
in the grammar in opposite order
▶ accept when only the start symbol (and blanks)
are left on the tape
▶ because the language is context-sensitive,
we never need additional space on the tape
(empty word needs special treatment)
</p>
<p><b></b>The languages that can be recognized by nondeterministic
Turing machines are exactly the type-0 languages.
Without proof.
proof sketch for grammar ⇒ NTM direction:
▶ analogous to previous proof
▶ for grammar rules w1 → w2 with |w1| > |w2|,
we must “insert” symbols into the existing tape content;
this is a bit tedious, but not very difficult
</p>
<p><b>Decidable vs. Turing-recognizable</b>
A language L is decidable iff both L and ¯L are Turing-recognizable.
</p>
<p><b></b>The halting problem H is Turing-recognizable.
The following Turing machine U recognizes language H:
On input w #x:
1
If the input contains more than one # then reject.
2 Simulate Mw (the TM encoded by w ) on input x.
3
If Mw halts, accept.
What does U do if Mw does not halt on the input?
U is an example of a so-called universal Turing machine
which can simulate any other Turing machine
from the description of that machine.
</p>
<p><b>Undecidability of the Halting Problem</b>
The halting problem H is undecidable.
</p>
<p><b></b>A language L ⊆ Σ∗ is decidable iff χL : Σ∗ → {0, 1},
the characteristic function of L, is computable.
Here, for all w ∈ Σ∗:
χL(w ) :=
(cid:40)
1
0
if w ∈ L
if w /∈ L
</p>
<p><b></b>A language L ⊆ Σ∗ is Turing-recognizable
iff the following function χ′
L : Σ∗ →p {0, 1} is computable.
Here, for all w ∈ Σ∗:
χ′
L(w ) =
(cid:40)
1
if w ∈ L
undefined if w ̸∈ L
</p>
<p><b>Reductions vs. Turing-recognizability/Decidability</b>
Let A and B be languages with A ≤ B. Then:
1
2
3
4
If B is decidable, then A is decidable.
If B is Turing-recognizable, then A is Turing-recognizable.
If A is not decidable, then B is not decidable.
If A is not Turing-recognizable, then B is not
Turing-recognizable.
⇝ In the following, we use 3. to show undecidability
for further problems.
</p>
<p><b>Reductions are Preorders</b>
The relation “≤” is a preorder:
1 For all languages A:
A ≤ A (reflexivity)
2 For all languages A, B, C :
If A ≤ B and B ≤ C , then A ≤ C (transitivity)
</p>
<p><b>Undecidability of Halting Problem on Empty Tape</b>
The halting problem on the empty tape is undecidable.
</p>
<p><b>Turing-recognizability of PCP</b>
PCP is Turing-recognizable.
</p>
<p><b>Undecidability of PCP</b>
PCP is undecidable.
</p>
<p><b>Rice’s Theorem</b>
Let R be the class of all computable partial functions.
Let S be an arbitrary subset of R except S = ∅ or S = R.
Then the language
C (S) = {w ∈ {0, 1}∗ | the (partial) function computed by Mw
is in S}
is undecidable.
</p>
<p><b>HamiltonCycle ≤p TSP</b>
HamiltonCycle ≤p TSP.
</p>
<p><b>Properties of Polynomial Reductions</b>
Let A, B and C decision problems.
1
2
3
4
5
If A ≤p B and B ∈ P, then A ∈ P.
If A ≤p B and B ∈ NP, then A ∈ NP.
If A ≤p B and A /∈ P, then B /∈ P.
If A ≤p B and A /∈ NP, then B /∈ NP.
If A ≤p B and B ≤p C , then A ≤p C .
</p>
<p><b>Proving NP-Completeness by Reduction</b>
Let A and B be problems such that:
▶ A is NP-hard, and
▶ A ≤p B.
Then B is also NP-hard.
If furthermore B ∈ NP, then B is NP-complete.
</p>
<p><b>Cook, 1971; Levin, 1973</b>
SAT is NP-complete.
</p>
<p><b>3SAT is NP-Complete</b>
3SAT is NP-complete.
</p>
<p><b>Clique is NP-Complete</b>
Clique is NP-complete.
</p>
<p><b>IndSet is NP-Complete</b>
IndSet is NP-complete.
</p>
<p><b>VertexCover is NP-Complete</b>
VertexCover is NP-complete.
</p>
<p><b></b>DirHamiltonCycle is NP-complete.
</p>
<p><b></b>HamiltonCycle is NP-complete.
</p>
<p><b></b>TSP is NP-complete.
</p>
<p><b></b>SubsetSum is NP-complete.
</p>
<p><b></b>Partition is NP-complete.
</p>
<p><b></b>BinPacking is NP-complete.
</p>
<p><b></b>Every LOOP-computable function is WHILE-computable.
The converse is not true.
WHILE programs are therefore strictly more powerful
than LOOP programs.
</p>
<p><b></b>The Ackermann function is WHILE-computable,
but not LOOP-computable.
(Without proof.)
</p>
<p><b></b>Every GOTO-computable function is WHILE-computable.
If we allow IF statements, a single WHILE loop is sufficient for this.
(We will discuss the converse statement later.)
</p>
<p><b></b>Every WHILE-computable function is Turing-computable.
(We will discuss the converse statement later.)
</p>
<p><b>Turing-Computability vs. GOTO-Computability</b>
Every Turing-computable numerical function is GOTO-computable.
</p>
</div>
</body></html>
